# emotion_analysis
소설 등장인물 감정분석

### 개요  
현대사회에서 인스타그램, 트위터 등 여러 sns를 통해 자신의 감정을 표출하고 있다. 게시물에서 작성자의 감정을 파악하는 것이 글을 이해하는 것에 있어 핵심이다. 그래야 답글을 남길때에 문맥에
벗어나지 않는 글을 작성할 수 있고 게시글에 공감할 수 있다. 카카오톡, 문자와 같은 메신저를 주고 받을때도 마찬가지이다.
트위터 10만개의 데이터셋에서 긍정과 부정 감정을 파악하여 이를 훈련시킨후 고전 문학에서 각 캐릭터의 특성이 잘 들어나 있는 작은아씨들에 적용하여 감정분석 정확도를 확인 하였다.
추후 카카오톡과 같은 단체 채팅방및 1대1 개인 채팅방에서 인물별 긍정,부정 표현 표시 예정 


### 처리 순서도
트윗 문장 전처리 -> Word2Vec모델 생성 및 훈련 -> 문자 Encoding -> 딥러닝(RNN-LSTM 등)모델 생성 및 훈련 -> 예측 및 정확도 확인 -> 작은아씨들 전처리 -> 작은아씨들 예측 및 정확도 확인


### 문장 전처리(preprocessing)
트윗에서 id,@,#등 특수문자 제거 후 text를 추출하여 전처리 진행 
- 불용어 처리

  a,the, I, we 등 문맥의 긍정, 부정 감정에 영향을 끼치지 않는 단어를 제거하며 기존 stopwords 불용어 사전에서 not,no,t 를 제외시키고 적용하여 no,doesn't,couldn't 등 부정 표현 단어를 학습할 수 있도록 하였다.
- 어간 추출(stem)

  SnowballStemmer을 사용, 단어의 복수형,과거형 등 시제변환을 기본형태로 추출하여 학습한다. ex)formalize->formal, exception->except


### Word2Vec
Word2Vector를 이용하여 단어의 유사단어 및 네거티브 샘플링 기능을 활용하였다. 
Word2Vec은 기존에 원-핫 인코딩은 벡터안에 단어의 의미가 들어가 있지 않기 때문에 단어간의 유사성을 계산할 수 없었던 것을 분산 표현 방법을 이용하여 단어의 '의미'를 다차원 공간에 벡터화 하였다.
문장에서 비슷한 위치에 등장하는 단어는 비슷한 의미를 갖는 것을 가설로 하여 코퍼스로 부터 단어들의 데이터셋을 학습하고, 벡터에 단어의 의미를 여러 차원에 분산하여 벡터로 표현한다.
이를 이용하여 주변단어들을 갖고오고 다른 상관없는 단어들의 일부만 갖고와 마지막에 이진 분류로 하여 긍정, 부정 분류를 한다. 즉 주변단어를 긍정이라 두고 랜덤으로 샘플링된 단어를 부정으로 둔 다음 이진분류 한다.
size=300으로 벡터크기 설정하였고, min_count=5 설정하여 최소 5개 이상 나오는 단어만 Word2Vec으로 추출 하도록하였다.

### 문자 Encoding
- 토큰화(tokenize)

  Tokenizer을 사용하여 트윗 전처리한 문자를 정수 인덱스벡터 형태로 변환한다.
- pad_sequence()

  maxlen=300을 다 못채우는 벡터 크기를 앞에서 부터 값을 0을 넣어 채워 모두 동일하게 maxlen 벡터 크기를 갖게 만든다.
- LabelEncoding

  LabelEncoder을 사용하여 문자를 숫자로 수치화 시킨다.

### 모델 생성 및 훈련
- RNN_LSTM

  RNN은 은닉층에 방향을 가진 엣지로 연결돼 순환구조를 이루는 인공 신경망으로 시퀀스 길이에 관계없이 input, ouput을 받아들일 수 있는 네트워크 구조이다.

  LSTM은 RNN이 갖고 있는 vanishing gradient problem(관련 정보와 그 정보를 사용하는 지점 사이가 멀 경우 역전파시 그래디언트가 줄어 학습능력저하)를 극복하기 위해 고안되었다. RNN의 은닉층에 cell-state를 추가한 구조로 state가 오래 지나도 그래디언트 전파가 잘 된다.
- Dropout

  네트워크의 일부를 생략하는 것으로 정규화(regularization) 효과를 갖는다. 동조화 되는 현상을 피할 수 있으며 response time의 시간을 줄여준다.
- Embedding layer

  입력 정수에 대해 밀집 벡터로 맵핑하고 인공 신경망의 학습 과정에서 가중치가 학습되는 방식으로 훈련된다. 즉 특정 단어와 맵핑되는 정수를 인덱스로 갖는 테이블로 부터 embedding 벡터 값을 갖는 룩업 테이블이다. 모든 단어는 고유한 임베딩 벡터를 갖는다.
- Dense

  입력과 출력을 모두 연결해주며 각 연결선에 가중치(weight)을 포함하여 연결강도를 나타낸다. 첫번째 인자는 출력 뉴련의 수를 설정, activation으로 활성화 함수를 설정한다. relu는 은익층에 주로 쓰이고 sigmoid는 이진 분류 출력층에, softmax는 다중 클래스 분류 출력층에 쓰인다.

- 모델 훈련 설정

  범주형 변수를 예측할며 카테고리가 2개 임으로 loss= binary_crossentropy를 설정 하였다. 

  optimizer(최적화)는  Adagrad + RMSProp의 장점을 섞어놓은 adam을 사용하였다. 즉 그레디언트, 학습률도 적당하게 한 것이다.

  Batch size가 클수록 그래디언트가 정확해지지만 한 iteration에 대한 계산량이 늘어나게 된다. 반면 너무 크면 오래전 값을 사용하여 부정확해진다. batch_size를 1024로 설정하였다. 

  epochs는 전체 데이터의 학습 횟수라 할 수 있다. 너무 크면 overfitting되고 너무 적으면 underfitting 된다.
